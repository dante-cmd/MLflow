{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"10px\">\n",
    "<img src=\"https://mlflow.org/docs/latest/images/logo-light.svg\" height=\"50px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Streamline*: to be more efficient, more effective or simplier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLflow** is tool for managing the Machine Learning Lifecycle\n",
    "\n",
    "- It's open-source a plattform\n",
    "- purpose-built to assist ML teams in handling the complexities of the ML process.\n",
    "- MLflow focuses on the full lifecycle for machine learning projects, ensuring  that each phase manageable, traceable, reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start a Tracking Server (local): \n",
    "   ```bash\n",
    "   mlflow server --host 127.0.0.1 --port 8080\n",
    "   ```\n",
    "2. Set the Tracking Server URI if not using Databricks\n",
    "   ```python\n",
    "   import mlflow\n",
    "   mlflow.set_tracking_uri(uri=\"http://<host>:<port>\")\n",
    "   ``` \n",
    "3. Train a model and prepare the data for logging \n",
    "   ```python\n",
    "   import mlflow\n",
    "   from mlflow.models import infer_signature\n",
    "   import pandas as pd\n",
    "   from sklearn import datasets\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.linear_model import LogisticRegression\n",
    "   from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "   # Load the Iris dataset\n",
    "   X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "   # Split the data into training and test sets\n",
    "   X_train, X_test, y_train, y_test = train_test_split(\n",
    "      X, y, test_size=0.2, random_state=42\n",
    "   )\n",
    "\n",
    "   # Define the model hyperparameters\n",
    "   params = {\n",
    "      \"solver\": \"lbfgs\",\n",
    "      \"max_iter\": 1000,\n",
    "      \"multi_class\": \"auto\",\n",
    "      \"random_state\": 8888,\n",
    "   }\n",
    "\n",
    "   # Train the model\n",
    "   lr = LogisticRegression(**params)\n",
    "   lr.fit(X_train, y_train)\n",
    "\n",
    "   # Predict on the test set\n",
    "   y_pred = lr.predict(X_test)\n",
    "\n",
    "   # Calculate metrics\n",
    "   accuracy = accuracy_score(y_test, y_pred)\n",
    "   ```\n",
    "4. Log the model and its metadata to MLflow: records the *model*, *performance metrics*, *paramaters*\n",
    "\n",
    "   ```python\n",
    "   # Create a new MLflow Experiment\n",
    "   mlflow.set_experiment(\"MLflow Quickstart\")\n",
    "\n",
    "   # Start an MLflow run\n",
    "   with mlflow.start_run():\n",
    "      # Log the hyperparameters\n",
    "      mlflow.log_params(params)\n",
    "\n",
    "      # Log the loss metric\n",
    "      mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "      # Set a tag that we can use to remind ourselves what this run was for\n",
    "      mlflow.set_tag(\"Training Info\", \"Basic LR model for iris data\")\n",
    "\n",
    "      # Infer the model signature\n",
    "      # used to define the input and output schema of a machine learning model\n",
    "      signature = infer_signature(X_train, lr.predict(X_train))\n",
    "\n",
    "      # Log the model\n",
    "      model_info = mlflow.sklearn.log_model(\n",
    "         sk_model=lr,\n",
    "         artifact_path=\"models/iris_model\",\n",
    "         signature=signature,\n",
    "         input_example=X_train,\n",
    "         registered_model_name=\"tracking-quickstart\",\n",
    "      )\n",
    "   ```\n",
    "5. Load the model as Python Function (pyfunc) and use it for inference. \n",
    "\n",
    "   ```python\n",
    "   # Load the model back for predictions as a generic Python Function model\n",
    "   loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "\n",
    "   predictions = loaded_model.predict(X_test)\n",
    "\n",
    "   iris_feature_names = datasets.load_iris().feature_names\n",
    "\n",
    "   result = pd.DataFrame(X_test, columns=iris_feature_names)\n",
    "   result[\"actual_class\"] = y_test\n",
    "   result[\"predicted_class\"] = predictions\n",
    "\n",
    "   result[:4]\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training block code is outside of `mlflow.start_run()`. If there are some issues about the code, we can solve before to log de model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four official ways to interact with MLflow\n",
    "\n",
    "There are at least 4 official ways to interact with MLflow.\n",
    "\n",
    "| Method            | Description            | Use Case                              |\n",
    "| ----------------- | ---------------------- | ------------------------------------- |\n",
    "| **Fluent API**    | High-level Python API  | Logging runs, metrics, models         |\n",
    "| **MLflow Client** | Lower-level Python API | Managing runs, experiments, artifacts |\n",
    "| **REST API**      | HTTP interface         | Custom apps, non-Python clients       |\n",
    "| **CLI**           | Command-line interface | Quick tasks or scripting pipelines    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "1. Start MLflow Tracking Server (or Client). The server should be listening\n",
    "2. Create experiment\n",
    "3. Crear run (nested for tune hyperparameters)\n",
    "4. Train the model using current hyperparameter\n",
    "5. Log params (dictionary for *Fluent API* and one (key, value) for Client)\n",
    "6. Log metrics (dictionary for *Fluent API* and one (key, value) for Client)\n",
    "7. Log model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow Client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import time\n",
    "from dataset import generate_apple_sales_data_with_promo_adjustment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Get data\n",
    "data = generate_apple_sales_data_with_promo_adjustment()\n",
    "\n",
    "data['trend'] = data.date.dt.year/data.date.dt.year.min()\n",
    "\n",
    "# drop irrelevant date field and target field\n",
    "X = data.drop(columns=[\"date\", \"demand\"])\n",
    "X = X.astype('float64')\n",
    "y = data[\"demand\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring the MLflow Tracking Client\n",
    "\n",
    "We now have a client interface to the tracking server that can both send data to and retrieve data from the tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize client\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# 2. Create or get experiment\n",
    "experiment_name = \"Client-API-Simulation\"\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# If not exists create it otherwise get the \n",
    "# id of the experiment\n",
    "if experiment is None:\n",
    "    experiment_id = client.create_experiment(experiment_name)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "# 3. Start a new run\n",
    "run = client.create_run(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Set parameters\n",
    "params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_samples_split\": 10,\n",
    "    \"min_samples_leaf\": 4,\n",
    "    \"bootstrap\": True,\n",
    "    \"oob_score\": False,\n",
    "    \"random_state\": 888,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Log parameters\n",
    "for name_param, value_param in params.items():\n",
    "    client.log_param(run.info.run_id, \n",
    "                     name_param, value_param)\n",
    "\n",
    "# 6. split in train and test\n",
    "time_series_split = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for step, (idx_train, idx_test) in enumerate(\n",
    "    time_series_split.split(X, y)):\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val =  X.iloc[idx_train], X.iloc[idx_test]\n",
    "    y_train, y_val = y[idx_train], y[idx_test]\n",
    "\n",
    "    # Train the RandomForestRegressor\n",
    "    rf = RandomForestRegressor(**params)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = rf.predict(X_val)\n",
    "\n",
    "\n",
    "    # Calculate error metrics\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    # Assemble the metrics we're going to write into a collection\n",
    "    metrics = {\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2}\n",
    "    \n",
    "    # 5. Log metrics\n",
    "    for name_metric, value_metric in metrics.items():\n",
    "        client.log_metric(\n",
    "            run.info.run_id, \n",
    "            name_metric, \n",
    "            value_metric,\n",
    "            step=step)\n",
    "    \n",
    "    # Save the model locally\n",
    "    local_model_path = f\"model/model_{step}.pkl\"\n",
    "    \n",
    "    os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "    with open(local_model_path, \"wb\") as f:\n",
    "        pickle.dump(rf, f)\n",
    "    \n",
    "    # Log the model using the MLflow Client API\n",
    "    artifact_path = \"model\"  # Subdirectory in the artifact repository\n",
    "    model_uri = f\"runs:/{run.info.run_id}/{artifact_path}\"  # URI for the logged model\n",
    "\n",
    "    # Log the model\n",
    "    client.log_artifact(run.info.run_id, local_model_path, artifact_path)\n",
    "\n",
    "    \n",
    "    # If we want we can register the model\n",
    "    # and apply teh version\n",
    "    # Register the model\n",
    "    registered_model_name = \"RandomForestModel\"\n",
    "    client.create_registered_model(registered_model_name)\n",
    "\n",
    "    # Create a new version of the model in the registry\n",
    "    client.create_model_version(\n",
    "        name=registered_model_name,\n",
    "        source=model_uri,\n",
    "        run_id=run.info.run_id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 0cd7e450f4954b8aaa6c485de2fbd3c8\n",
      "Experiment ID: 424303544545129281\n",
      "Run completed and logged using Client API.\n"
     ]
    }
   ],
   "source": [
    "# Output run info\n",
    "print(\"Run ID:\", run.info.run_id)\n",
    "print(\"Experiment ID:\", experiment_id)\n",
    "print(\"Run completed and logged using Client API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More About ML Client**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default *MLflow Tracking Server* includes a *Default Experiment*, and this is used to save all information about experiment not declarated.\n",
    "\n",
    "This is useful when we forget to create a new experiment before using the MLflow traking capabilities(log, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Experiments**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tags and experiments\n",
    "\n",
    "If we run using the same input dataset, logically they belong to the same experiment, all the metadata (about of dataset) is filled in tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://mlflow.org/docs/latest/assets/images/tag-exp-run-relationship-fc898eccc4bb05fe59f41372ab5f6b50.svg\" height=\"300\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment description\n",
    "experiment_description = (\n",
    "    \"This is the grocery forecasting project.\"\n",
    "    \"This experiment contains the produce models for apples.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of\n",
    "# Runs that will be included in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\":\"grocery-forecasting\",\n",
    "    \"store_dept\":\"produce\",\n",
    "    \"team\":\"stores-ml\",\n",
    "    \"project_quarter\":\"Q3-2023\",\n",
    "    \"mlflow.note.content\":experiment_description\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "produce_apples_experiment = client.create_experiment(\n",
    "    name='Apples_Models', \n",
    "    tags=experiment_tags\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search Experiments**\n",
    "\n",
    "We can search the experiments that has the same project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='mlflow-artifacts:/446874737954528824', creation_time=1746984158981, experiment_id='446874737954528824', last_update_time=1746998852686, lifecycle_stage='active', name='Apples_Models', tags={'mlflow.note.content': 'This is the grocery forecasting project.This '\n",
       "                         'experiment contains the produce models for apples.',\n",
       "  'project_name': 'grocery-forecasting',\n",
       "  'project_quarter': 'Q3-2023',\n",
       "  'store_dept': 'produce',\n",
       "  'team': 'stores-ml'}>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search all experiments\n",
    "all_experiments = client.search_experiments()\n",
    "\n",
    "# Search experiments based on `project_name`\n",
    "client.search_experiments(\n",
    "    filter_string=\"tags.`project_name`='grocery-forecasting'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delete Experiments**\n",
    "\n",
    "Soft delete experiment. The experiment is not permanently removed from  the backend store it is marked as deleted and becomes hidden in the MLflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 614683981825209504\n",
      "Experiment: 347573977917312028\n",
      "Experiment: 446874737954528824\n"
     ]
    }
   ],
   "source": [
    "# Delete experiment\n",
    "client.delete_experiment(\"446874737954528824\")\n",
    "\n",
    "# Show all deleted experiments\n",
    "deleted_experiments = client.search_experiments(view_type=\"DELETED_ONLY\")\n",
    "\n",
    "for deleted_experiment in deleted_experiments:\n",
    "    print(\"Experiment: {}\".format(deleted_experiment.experiment_id))\n",
    "\n",
    "# We can restore the experiement use restore_experiment\n",
    "client.restore_experiment(\"446874737954528824\")\n",
    "\n",
    "# We can modify the key: value of the tag of the experiment\n",
    "# If the key not exists, it will be created.\n",
    "client.set_experiment_tag(\"446874737954528824\", 'project_name', 'grocery-forecasting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluent API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logging our first runs with MLflow**\n",
    "\n",
    "The code below is the final experiment after many attemps (all old experiments was deleted)\n",
    "\n",
    "We are going to use `Fluent` API. The `fluent` APIs use a globally referenced state of the MLflow tracking server's uri. \n",
    "\n",
    "This global instance allows for us to use these 'higher-level' (simpler) APIs to perform every action that we can otherwise do with the `MlflowClient`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four main components\n",
    "\n",
    "- **MLflow Traking**: Logs parameters, metrics, artifacts() for experiments:\n",
    "  - It allow log and *query* experiments. If MLflow run in local server all the metadata and artifact will be stored locally \n",
    "- **MLflow Project**: Package code and dependencies for reproducibility \n",
    "- **MLflow Models**: Standarizes model packaging and deployment.\n",
    "- **MLflow Model Registry**: Manage model versions and stages(e.g. development, production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Artifact** are files or directories associated with a run, such as trained models, serialized objects, visualizations, datasets, or any other outputs generated during an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "1. Start MLflow Tracking Server (or Client). The server should be listening\n",
    "2. Create experiment\n",
    "3. Crear run (nested for tune hyperparameters)\n",
    "4. Train the model using current hyperparameter\n",
    "5. Log params (dictionary for *Fluent API* and one (key, value) for Client)\n",
    "6. Log metrics (dictionary for *Fluent API* and one (key, value) for Client)\n",
    "7. Log model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start the MLflow Tracking Server without specifying a backend store or artifact root:\n",
    "- *Metadata* : Stored in `./mlruns` on the machine running the server.\n",
    "- *Artifacts* : Stored in `./mlruns` on the machine running the server.\n",
    "\n",
    "When we set *URI=\"http://127.0.0.1:8080\"*, all the metadata and artifacts about experiment will be stored in the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Custom Backend Store and Artifact Root would be\n",
    "\n",
    "```bash\n",
    "mlflow server \\\n",
    "    --backend-store-uri sqlite:///mlflow.db \\\n",
    "    --default-artifact-root s3://my-bucket/mlflow \\\n",
    "    --host 127.0.0.1 \\\n",
    "    --port 8080\n",
    "```\n",
    "\n",
    "- **Metadata** : Stored in the `mlflow.db` SQLite file on the machine running the server.\n",
    "- **Artifacts** : Stored in the `s3://my-bucket/mlflow bucket`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the current active experiment to the \"Apple_Models\" experiment and\n",
    "# returns the Experiment metadata\n",
    "apple_experiment = mlflow.set_experiment(\"Apples_Models\")\n",
    "\n",
    "# Define a run name for this iteration of training.\n",
    "# If this is not set, a unique name will be auto-generated for your run.\n",
    "run_name = \"apples_rf_test\"\n",
    "\n",
    "# Define an artifact path that the outputs of the experiments (models, datasets, ...) will be saved to.\n",
    "artifact_path = \"model/rf_apples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='mlflow-artifacts:/446874737954528824', creation_time=1746984158981, experiment_id='446874737954528824', last_update_time=1746998852686, lifecycle_stage='active', name='Apples_Models', tags={'mlflow.note.content': 'This is the grocery forecasting project.This '\n",
       "                         'experiment contains the produce models for apples.',\n",
       "  'project_name': 'grocery-forecasting',\n",
       "  'project_quarter': 'Q3-2023',\n",
       "  'store_dept': 'produce',\n",
       "  'team': 'stores-ml'}>,\n",
       " <Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1746920722099, experiment_id='0', last_update_time=1746920722099, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.search_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y,test_size=0.1)\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_samples_split\": 10,\n",
    "    \"min_samples_leaf\": 4,\n",
    "    \"bootstrap\": True,\n",
    "    \"oob_score\": False,\n",
    "    \"random_state\": 888,\n",
    "}\n",
    "\n",
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607753f3a627425d86f022767ba60913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run apples_rf_test at: http://127.0.0.1:8080/#/experiments/446874737954528824/runs/80df25d043e143618c2532bd8b6ffdd2\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/446874737954528824\n"
     ]
    }
   ],
   "source": [
    "# Initiate the MLflow run context\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    # Log the parameters used for the model fit\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log the error metrics that were calculated during validation\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Log an instance of the trained model for later use\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=rf, \n",
    "        input_example=X_val, \n",
    "        artifact_path=artifact_path,\n",
    "        signature=signature\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLflow Nested Runs**\n",
    "\n",
    "The function starts a new nested run in MLflow. Nested runs are useful for organizing hyperparameter tuning experiments as they allow you to group individual runs under a parent run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dante\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dfd62905b84dfeba53b494005d64a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run efficient-sow-531 at: http://127.0.0.1:8080/#/experiments/446874737954528824/runs/90f2c3ba83a84ae3b8622fcae7f07f1f\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/446874737954528824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dante\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fad7154c644c6d82da4352cb5df93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run blushing-mouse-769 at: http://127.0.0.1:8080/#/experiments/446874737954528824/runs/c463b38a1da346f6a1296fc2efdb7800\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/446874737954528824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dante\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47c0aba06a04cc4a2273986f850a5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run shivering-frog-724 at: http://127.0.0.1:8080/#/experiments/446874737954528824/runs/b9086db451ea4d658e01133a4af0160b\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/446874737954528824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dante\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2295e2e44e54ad4b94fda79d60c1973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run rogue-shoat-513 at: http://127.0.0.1:8080/#/experiments/446874737954528824/runs/ca4c1546743a4c38bcdff88193087806\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/446874737954528824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dante\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c53ab58eeb426daa263a82e6590226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run luminous-fowl-963 at: http://127.0.0.1:8080/#/experiments/446874737954528824/runs/cd1e9e85420848f2a5ceb78aa69935d0\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/446874737954528824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dante\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e56759759c64038884496c61aebf5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run thundering-fowl-902 at: http://127.0.0.1:8080/#/experiments/446874737954528824/runs/ccf44ca9ddc64574893c0a6e6a91c08b\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/446874737954528824\n"
     ]
    }
   ],
   "source": [
    "def train(params):\n",
    "    # Train the RandomForestRegressor\n",
    "    rf = RandomForestRegressor(**params)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = rf.predict(X_val)\n",
    "\n",
    "    # Calculate error metrics\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    # Assemble the metrics we're going to write into a collection\n",
    "    metrics = {\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2}\n",
    "    \n",
    "    return rf, metrics, X_train, y_train\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [6, 10, 15],\n",
    "    \"min_samples_split\": [10],\n",
    "    \"min_samples_leaf\": [4],\n",
    "    \"bootstrap\": [True],\n",
    "    \"oob_score\": [False],\n",
    "    \"random_state\": [888]}\n",
    "# Initiate the MLflow run context\n",
    "\n",
    "\n",
    "for value_params in product(*hyperparameters.values()):\n",
    "    with mlflow.start_run(nested = True):\n",
    "        params = dict(zip(hyperparameters.keys(), value_params))\n",
    "        rf, metrics, X_train, y_train = train(params)\n",
    "\n",
    "        # Get the schema of the data\n",
    "        signature = infer_signature(X_train, y_train)\n",
    "        \n",
    "        # Log the parameters used for the model fit\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # Log the error metrics that were calculated during validation\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # Log an instance of the trained model for later use\n",
    "        model_info = mlflow.sklearn.log_model(\n",
    "            sk_model=rf, \n",
    "            input_example=X_train, \n",
    "            artifact_path=artifact_path,\n",
    "            signature=signature\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search all runs related to experiment id\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[apple_experiment.experiment_id], \n",
    "    output_format='pandas'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_model = runs.loc[(runs.status == 'FINISHED') & \n",
    "                    (runs['metrics.mae'] == runs['metrics.mae'].min()),\n",
    "                    \"run_id\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = 'runs:/{}/{}'.format(id_model, artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c8c6aa4a93452583fe35b0e855422f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sklearn_pyfunc  = mlflow.sklearn.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = sklearn_pyfunc.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ml."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
